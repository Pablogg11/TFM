{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "import torch\n",
    "import numpy as np\n",
    "from openxai.experiment_utils import print_summary, load_config, fill_param_dict\n",
    "from openxai.explainers.perturbation_methods import get_perturb_method\n",
    "\n",
    "# ML models\n",
    "from openxai.model import train_model\n",
    "\n",
    "# Data loaders\n",
    "from openxai.dataloader import ReturnLoaders, ReturnTrainTestX\n",
    "\n",
    "# Explanation models\n",
    "from openxai.explainer import Explainer\n",
    "\n",
    "# Evaluation methods\n",
    "from openxai.evaluator import Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the model and the data set you wish to generate explanations for\n",
    "n_test_samples = 10\n",
    "data_name = 'spambase' # must be one of ['adult', 'compas', 'gaussian', 'german', 'gmsc', 'heart', 'heloc', 'pima']\n",
    "model_name = 'ann'    # must be one of ['lr', 'ann']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and test loaders\n",
    "trainloader, testloader = ReturnLoaders(data_name=data_name, batch_size=n_test_samples)\n",
    "inputs, labels = next(iter(testloader))\n",
    "labels = labels.type(torch.int64)\n",
    "\n",
    "# Get full train/test FloatTensors and feature metadata\n",
    "X_train, X_test, feature_metadata = ReturnTrainTestX(data_name, float_tensor=True, return_feature_metadata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training lr on spambase dataset\n",
      "6 20.93 Best Seen Test Acc (Mean Pred = 0.79)\n",
      "7 23.46 Best Seen Test Acc (Mean Pred = 0.77)\n",
      "8 25.42 Best Seen Test Acc (Mean Pred = 0.75)\n",
      "9 27.3 Best Seen Test Acc (Mean Pred = 0.73)\n",
      "10 28.96 Best Seen Test Acc (Mean Pred = 0.71)\n",
      "11 30.41 Best Seen Test Acc (Mean Pred = 0.7)\n",
      "12 32.8 Best Seen Test Acc (Mean Pred = 0.67)\n",
      "13 33.74 Best Seen Test Acc (Mean Pred = 0.66)\n",
      "14 37.8 Best Seen Test Acc (Mean Pred = 0.62)\n",
      "15 40.12 Best Seen Test Acc (Mean Pred = 0.6)\n",
      "16 41.13 Best Seen Test Acc (Mean Pred = 0.59)\n",
      "17 42.0 Best Seen Test Acc (Mean Pred = 0.58)\n",
      "18 43.01 Best Seen Test Acc (Mean Pred = 0.57)\n",
      "19 44.9 Best Seen Test Acc (Mean Pred = 0.55)\n",
      "Proportion of Class 1:\n",
      "\tTest Preds:\t0.5510\n",
      "\tTest Set:\t0.0000\n",
      "Test Accuracy: 0.4490\n",
      "Train Accuracy: 0.8699\n",
      "File saved to models/LogisticRegression/spambase_lr_minmax_ep_20_lr_0.001_batch_32_seed_0_pcw_0.55_mpb_0.15_wu_5_acc_44.9_at_ep_19.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from openxai.model import train_model\n",
    "from openxai.experiment_utils import load_config\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = load_config('experiment_config.json')\n",
    "    model_names, data_names = config['model_names'], config['data_names']\n",
    "    train_config = config['training']\n",
    "    epochs, learning_rate = train_config['epochs'], train_config['learning_rate']\n",
    "    scaler, seed, warmup = train_config['scaler'], train_config['seed'], train_config['warmup']\n",
    "    model_name = \"lr\"\n",
    "    data_name = \"spambase\"\n",
    "    print(f'Training {model_name} on {data_name} dataset')\n",
    "\n",
    "    # Train Model\n",
    "    data_config = train_config[data_name]\n",
    "    batch, pcw, mpb = data_config['batch_size'], data_config['pos_class_weight'], data_config['mean_pred_bound']\n",
    "    model, best_acc, best_epoch = train_model(model_name, data_name, learning_rate, epochs, batch,\n",
    "                                                      scaler=scaler, seed=seed, pos_class_weight=pcw,\n",
    "                                                      mean_prediction_bound=mpb, warmup=warmup, verbose=False)\n",
    "            \n",
    "    # Save Model\n",
    "    params = {'ep': epochs, 'lr': learning_rate, 'batch': batch, 'seed': seed, 'pcw': pcw,\n",
    "                      'mpb': mpb, 'wu': warmup, 'acc': str(round(best_acc*100, 2)), 'at_ep': best_epoch}\n",
    "    params_str = '_'.join([f'{k}_{v}' for k, v in params.items()])\n",
    "    model_file_name = f'{data_name}_{model_name}_{scaler}_{params_str}.pt'\n",
    "    model_folder_name = f'models/{model.name}/'\n",
    "    if not os.path.exists(model_folder_name):\n",
    "            os.makedirs(model_folder_name)\n",
    "    torch.save(model.state_dict(),  model_folder_name + model_file_name)\n",
    "    print(f'File saved to {model_folder_name + model_file_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIME Parameters\n",
      "\n",
      "data: array of size torch.Size([3220, 57])\n",
      "Remaining parameters are set to their default values\n"
     ]
    }
   ],
   "source": [
    "# Choose explainer\n",
    "method = 'lime'\n",
    "\n",
    "# Pass empty dict to use default parameters\n",
    "param_dict = {}\n",
    "\n",
    "# # If LIME/IG, then provide X_train\n",
    "param_dict = fill_param_dict(method, {}, X_train)\n",
    "params_preview = [f'{k}: array of size {v.shape}' if hasattr(v, 'shape') else f'{k}: {v}' for k, v in param_dict.items()]\n",
    "print(f'{method.upper()} Parameters\\n\\n' +'\\n'.join(params_preview))\n",
    "print('Remaining parameters are set to their default values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.17815466 -0.22081512  0.2289229   0.17844062  0.32499242  0.3181003\n",
      "  0.41927537  0.35710293  0.2693681   0.23881984  0.33944052 -0.12253211\n",
      "  0.21121967  0.08442789  0.27314356  0.42983812  0.33861926  0.34939814\n",
      "  0.34912372  0.35127473  0.35858402  0.27702907  0.38679504  0.3436327\n",
      " -0.4400574  -0.40359542 -0.4264144  -0.2833649  -0.27130625 -0.35710195\n",
      " -0.2935212  -0.27716857 -0.35203025 -0.22850059 -0.30423355 -0.24596947\n",
      " -0.32401863 -0.05644296 -0.35636088 -0.05011095 -0.23789021 -0.37307262\n",
      " -0.32942083 -0.27428567 -0.27408767 -0.2209086  -0.18799941 -0.37666097\n",
      " -0.12133754 -0.18904619 -0.23224598  0.4233583   0.36191273  0.18612024\n",
      "  0.20406231  0.3286438   0.2951493 ]\n"
     ]
    }
   ],
   "source": [
    "# Compute explanations\n",
    "preds = model(inputs.float()).argmax(1)\n",
    "lime = Explainer(method, model, param_dict)\n",
    "lime_exps = lime.get_explanations(inputs.float(), preds).detach().numpy()\n",
    "print(lime_exps[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth metrics:  ['PRA', 'RC', 'FA', 'RA', 'SA', 'SRA']\n",
      "Prediction metrics:  ['PGU', 'PGI']\n",
      "Stability metrics:  ['RIS', 'RRS', 'ROS']\n"
     ]
    }
   ],
   "source": [
    "from openxai.evaluator import ground_truth_metrics, prediction_metrics, stability_metrics\n",
    "print('Ground truth metrics: ', ground_truth_metrics)\n",
    "print('Prediction metrics: ', prediction_metrics)\n",
    "print('Stability metrics: ', stability_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRA Parameters\n",
      "\n",
      "explanations: array of size (10, 57)\n"
     ]
    }
   ],
   "source": [
    "# Choose one of ['PRA', 'RC', 'FA', 'RA', 'SA', 'SRA']\n",
    "metric = 'PRA'  \n",
    "\n",
    "# Load config\n",
    "param_dict = load_config('experiment_config.json')['evaluators']['ground_truth_metrics']\n",
    "param_dict['explanations'] = lime_exps\n",
    "if metric in ['FA', 'RA', 'SA', 'SRA']:\n",
    "    param_dict['predictions'] = preds  # flips ground truth according to prediction\n",
    "elif metric in ['PRA', 'RC']:\n",
    "    del param_dict['k'], param_dict['AUC']  # not needed for PRA/RC\n",
    "\n",
    "# Print final parameters\n",
    "params_preview = [f'{k}: array of size {v.shape}' if hasattr(v, 'shape') else f'{k}: {v}' for k, v in param_dict.items()]\n",
    "print(f'{metric.upper()} Parameters\\n\\n' +'\\n'.join(params_preview))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the metric across the test inputs/explanations\n",
    "evaluator = Evaluator(model, metric)\n",
    "score, mean_score = evaluator.evaluate(**param_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRA: 0.90Â±0.02\n"
     ]
    }
   ],
   "source": [
    "std_err = np.std(score) / np.sqrt(len(score))\n",
    "print(f\"{metric}: {mean_score:.2f}\\u00B1{std_err:.2f}\")\n",
    "if metric in stability_metrics:\n",
    "    log_mu, log_std = np.log(mean_score), np.log(std_err)\n",
    "    print(f\"log({metric}): {log_mu:.2f}\\u00B1{log_std:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
