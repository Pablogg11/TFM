{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pgarc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Utils\n",
    "import torch\n",
    "import numpy as np\n",
    "from openxai.experiment_utils import print_summary, load_config, fill_param_dict\n",
    "from openxai.explainers.perturbation_methods import get_perturb_method\n",
    "\n",
    "# ML models\n",
    "from openxai.model import train_model\n",
    "\n",
    "# Data loaders\n",
    "from openxai.dataloader import ReturnLoaders, ReturnTrainTestX\n",
    "\n",
    "# Explanation models\n",
    "from openxai.explainer import Explainer\n",
    "\n",
    "# Evaluation methods\n",
    "from openxai.evaluator import Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the model and the data set you wish to generate explanations for\n",
    "n_test_samples = 10\n",
    "data_name = 'spambase' # must be one of ['adult', 'compas', 'gaussian', 'german', 'gmsc', 'heart', 'heloc', 'pima']\n",
    "model_name = 'ann'    # must be one of ['lr', 'ann']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and test loaders\n",
    "trainloader, testloader = ReturnLoaders(data_name=data_name, batch_size=n_test_samples)\n",
    "inputs, labels = next(iter(testloader))\n",
    "labels = labels.type(torch.int64)\n",
    "\n",
    "# Get full train/test FloatTensors and feature metadata\n",
    "X_train, X_test, feature_metadata = ReturnTrainTestX(data_name, float_tensor=True, return_feature_metadata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ann on spambase dataset\n",
      "6 69.51 Best Seen Test Acc (Mean Pred = 0.3)\n",
      "7 78.2 Best Seen Test Acc (Mean Pred = 0.22)\n",
      "Proportion of Class 1:\n",
      "\tTest Preds:\t0.2180\n",
      "\tTest Set:\t0.0000\n",
      "Test Accuracy: 0.7820\n",
      "Train Accuracy: 0.9432\n",
      "File saved to models/ArtificialNeuralNetwork/spambase_ann_minmax_ep_20_lr_0.001_batch_32_seed_0_pcw_0.55_mpb_0.15_wu_5_acc_78.2_at_ep_7.pt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openxai.model import train_model\n",
    "from openxai.experiment_utils import load_config\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = load_config('experiment_config.json')\n",
    "    model_names, data_names = config['model_names'], config['data_names']\n",
    "    train_config = config['training']\n",
    "    epochs, learning_rate = train_config['epochs'], train_config['learning_rate']\n",
    "    scaler, seed, warmup = train_config['scaler'], train_config['seed'], train_config['warmup']\n",
    "    model_name = \"lr\"\n",
    "    data_name = \"spambase\"\n",
    "    print(f'Training {model_name} on {data_name} dataset')\n",
    "\n",
    "    # Train Model\n",
    "    data_config = train_config[data_name]\n",
    "    batch, pcw, mpb = data_config['batch_size'], data_config['pos_class_weight'], data_config['mean_pred_bound']\n",
    "    model, best_acc, best_epoch = train_model(model_name, data_name, learning_rate, epochs, batch,\n",
    "                                                      scaler=scaler, seed=seed, pos_class_weight=pcw,\n",
    "                                                      mean_prediction_bound=mpb, warmup=warmup, verbose=False)\n",
    "            \n",
    "    # Save Model\n",
    "    params = {'ep': epochs, 'lr': learning_rate, 'batch': batch, 'seed': seed, 'pcw': pcw,\n",
    "                      'mpb': mpb, 'wu': warmup, 'acc': str(round(best_acc*100, 2)), 'at_ep': best_epoch}\n",
    "    params_str = '_'.join([f'{k}_{v}' for k, v in params.items()])\n",
    "    model_file_name = f'{data_name}_{model_name}_{scaler}_{params_str}.pt'\n",
    "    model_folder_name = f'models/{model.name}/'\n",
    "    if not os.path.exists(model_folder_name):\n",
    "            os.makedirs(model_folder_name)\n",
    "    torch.save(model.state_dict(),  model_folder_name + model_file_name)\n",
    "    print(f'File saved to {model_folder_name + model_file_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIME Parameters\n",
      "\n",
      "data: array of size torch.Size([3220, 57])\n",
      "Remaining parameters are set to their default values\n"
     ]
    }
   ],
   "source": [
    "# Choose explainer\n",
    "method = 'lime'\n",
    "\n",
    "# Pass empty dict to use default parameters\n",
    "param_dict = {}\n",
    "\n",
    "# # If LIME/IG, then provide X_train\n",
    "param_dict = fill_param_dict(method, {}, X_train)\n",
    "params_preview = [f'{k}: array of size {v.shape}' if hasattr(v, 'shape') else f'{k}: {v}' for k, v in param_dict.items()]\n",
    "print(f'{method.upper()} Parameters\\n\\n' +'\\n'.join(params_preview))\n",
    "print('Remaining parameters are set to their default values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.05141808  0.03493157  0.16536704 -0.26003003 -0.09243292  0.00152439\n",
      " -0.7897435  -0.45802286 -0.18733783 -0.32081455 -0.12736195  0.12263069\n",
      "  0.03992029 -0.17215899 -0.5311434  -0.84255403 -0.11506901 -0.19265045\n",
      "  0.04535412 -0.45205206 -0.18451896 -0.5715987  -0.84387714 -0.8328402\n",
      "  0.94846255  0.47635293  1.2275392   0.25440198  0.4573849   0.3408685\n",
      "  0.8538546   0.30144352  0.11264968  0.4697578   0.26006842 -0.15625691\n",
      " -0.03723517  0.03762506  0.16297458  0.06101436  0.6319182   0.7767345\n",
      "  0.06834373  0.6997992   0.33897105  0.26266935  0.23242605  0.61182725\n",
      " -0.0302032   0.08369949  0.13386141 -0.7826016  -1.0388671  -0.364889\n",
      " -0.09028549 -0.27863923 -0.24770932]\n"
     ]
    }
   ],
   "source": [
    "# Compute explanations\n",
    "preds = model(inputs.float()).argmax(1)\n",
    "lime = Explainer(method, model, param_dict)\n",
    "lime_exps = lime.get_explanations(inputs.float(), preds).detach().numpy()\n",
    "print(lime_exps[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth metrics:  ['PRA', 'RC', 'FA', 'RA', 'SA', 'SRA']\n",
      "Prediction metrics:  ['PGU', 'PGI']\n",
      "Stability metrics:  ['RIS', 'RRS', 'ROS']\n"
     ]
    }
   ],
   "source": [
    "from openxai.evaluator import ground_truth_metrics, prediction_metrics, stability_metrics\n",
    "print('Ground truth metrics: ', ground_truth_metrics)\n",
    "print('Prediction metrics: ', prediction_metrics)\n",
    "print('Stability metrics: ', stability_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRA Parameters\n",
      "\n",
      "explanations: array of size (10, 57)\n"
     ]
    }
   ],
   "source": [
    "# Choose one of ['PRA', 'RC', 'FA', 'RA', 'SA', 'SRA']\n",
    "metric = 'PRA'  \n",
    "\n",
    "# Load config\n",
    "param_dict = load_config('experiment_config.json')['evaluators']['ground_truth_metrics']\n",
    "param_dict['explanations'] = lime_exps\n",
    "if metric in ['FA', 'RA', 'SA', 'SRA']:\n",
    "    param_dict['predictions'] = preds  # flips ground truth according to prediction\n",
    "elif metric in ['PRA', 'RC']:\n",
    "    del param_dict['k'], param_dict['AUC']  # not needed for PRA/RC\n",
    "\n",
    "# Print final parameters\n",
    "params_preview = [f'{k}: array of size {v.shape}' if hasattr(v, 'shape') else f'{k}: {v}' for k, v in param_dict.items()]\n",
    "print(f'{metric.upper()} Parameters\\n\\n' +'\\n'.join(params_preview))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The metric PRA is incompatible with non-linear models.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Evaluate the metric across the test inputs/explanations\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m \u001b[43mEvaluator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m score, mean_score \u001b[38;5;241m=\u001b[39m evaluator\u001b[38;5;241m.\u001b[39mevaluate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparam_dict)\n",
      "File \u001b[1;32mc:\\Users\\pgarc\\Documents\\UPM\\MUIA\\MÃ¡ster\\TFM\\GitHub\\TFM\\openxai\\evaluator.py:52\u001b[0m, in \u001b[0;36mEvaluator.__init__\u001b[1;34m(self, model, metric)\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mground_truth\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mreturn_ground_truth_importance()\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe metric \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is incompatible with non-linear models.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Set stability/prediction metric parameters\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metric \u001b[38;5;129;01min\u001b[39;00m stability_metrics \u001b[38;5;241m+\u001b[39m prediction_metrics:\n",
      "\u001b[1;31mValueError\u001b[0m: The metric PRA is incompatible with non-linear models."
     ]
    }
   ],
   "source": [
    "# Evaluate the metric across the test inputs/explanations\n",
    "evaluator = Evaluator(model, metric)\n",
    "score, mean_score = evaluator.evaluate(**param_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRA: 0.90Â±0.02\n"
     ]
    }
   ],
   "source": [
    "std_err = np.std(score) / np.sqrt(len(score))\n",
    "print(f\"{metric}: {mean_score:.2f}\\u00B1{std_err:.2f}\")\n",
    "if metric in stability_metrics:\n",
    "    log_mu, log_std = np.log(mean_score), np.log(std_err)\n",
    "    print(f\"log({metric}): {log_mu:.2f}\\u00B1{log_std:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
